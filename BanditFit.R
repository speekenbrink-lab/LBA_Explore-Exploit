#Task which Fits the bandit task
BanditFit <- function(params, rawData, whichLBA, learnType, bonusStyle){
  #Basic Details----
  trialNum <- nrow(rawData)
  rewardSD <- 4
  cond <- rawData$cond[1]
    #For the addUncs function
  test <- data.frame(choice = rawData$deck, rewards = rawData$payoff)
  
  if (any(params[1:4] > params[5])){
    NLL <-  100000
    return(NLL)
  }
  
  #Assign params to values ----
  #A is the start-point for each option, let all As be different initially to account for bias
  A = rep(0, 4)
  A[1] <- params[1]
  A[2] <- params[2]
  A[3] <- params[3]
  A[4] <- params[4]
  
  #B is the bound. To keep things identifiable, make the bound set across all arms
  B <- rep(params[5], 4)
  
  #T0 is the amount of Non-decision time
  T0 <- params[6]
  
  #THETA is the temperature parameter for all models
  THETA <- params[7]
  
  #QSTART is a free parameter and helps determine whether participants will be optimistic or not at the beginning by determining starting values
  QSTART <- rep(params[8], 4)
  
  #ZETA is the prospect theory power
  ZETA <- params[9]
  
  #DELTA is the loss-aversion parameter
  DELTA <- params[10]
  
  #ALPHA_POS is the learning rate parameter for RW in the positive direction (Pedersen et al., 2017)
  ALPHA_POS <- params[11]
  #ALPHA_NEG is the learning rate parameter for RW in the negative direction (Pedersen et al., 2017)
  ALPHA_NEG <- params[12]
  
  #SD_V is the drift-rate parameter, again we'll keep this constant (unless we're testing a model where we alter it)
  SD_V <- rep(params[13], 4)
  
  #UNCSTART is where uncertainty expectations start at 
  UNCSTART <- rep(params[14], 4)
  
  #GAMMA is a general parameter that transforms uncertainty (although it will act differently in different models)
  GAMMA <- params[15]
  
  #Complete the learning model ----
  #Qs are for EVs
  Qs <- QSTART
  QTable <- matrix(ncol = 4, nrow = trialNum)
  #S is for Kalman uncertainty
  S <- rep(1000, 4)
  STable <- matrix(ncol = 4, nrow = trialNum)
  #See what values have been observed on which options
  valuesObserved <- matrix(ncol = 4, nrow = trialNum)
  #Uncertainty tracker
  Uncs <- UNCSTART
  tempUncs <- UNCSTART
  UncTable <- matrix(ncol = 4, nrow = trialNum)
  tempUncTable <- matrix(ncol = 4, nrow = trialNum)
  
  #RW Learning ----
  if (learnType == 'RW') {
    for (t in 1:trialNum) {
      QTable[t, 1:4] <- Qs
      STable[t, 1:4] <- S
      UncTable[t, 1:4] <- Uncs
      tempUncTable[t, 1:4] <- tempUncs
      
      choice <- rep(0, times = length(Qs))
      choice[rawData$deck[t]] <- 1
      
      if (rawData$payoff[t] >= 0) {
        utility <- rawData$payoff[t] ^ ZETA
        Qs[1:4] <- RW(ALPHA_POS, Qs[1:4], utility, choice)
      } else {
        utility <- -DELTA * ((abs(rawData$payoff[t])) ^ ZETA)
        Qs[1:4] <- RW(ALPHA_NEG, Qs[1:4], utility, choice)
      }
      
      #Question, should we extract our SDs from observed rewards or transformed rewards? I've used transformed rewards here
      valuesObserved[t, rawData$deck[t]] <- utility
      
      #Update uncertainty
      if (whichLBA != 5){
        Uncs <- addUnc(bonusStyle, Uncs, test, utility, t, valuesObserved, Qs, UNCSTART[1])
      } else if (whichLBA == 5) {
        Uncs <- addUnc(1, Uncs, test, utility, t, valuesObserved, Qs, UNCSTART[1])
        tempUncs <- addUnc(3, Uncs, test, utility, t, valuesObserved, Qs, UNCSTART[1])
      }
      
    }
    
  #Kalman----
  } else if (learnType == 'Kalman') {
    for (t in 1:trialNum) {
      QTable[t, 1:4] <- Qs
      STable[t, 1:4] <- S
      UncTable[t, 1:4] <- Uncs
      tempUncTable[t, 1:4] <- tempUncs
      
      choice <- rep(0, times = length(Qs))
      choice[rawData$deck[t]] <- 1
      
      if (rawData$payoff[t] >= 0) {
        utility <- rawData$payoff[t] ^ ZETA
      } else {
        utility <- -DELTA * ((abs(rawData$payoff[t])) ^ ZETA)
      }
      
      if ((cond == 'ntn' & t > 50 & t < 101) | 
          (cond == 'ntn' & t > 150 & t < 201) | 
          (cond == 'nts' & t > 50 & t < 101) | 
          (cond == 'nts' & t > 150 & t < 201)){
        walkSD <- 16
      } else {
        walkSD <- 4
      }
      
      outList <- KalmanFilter(Qs, S, utility, choice, walkSD, rewardSD)
      Qs <- outList$Qs
      S <- outList$S
      
      #Question, should we extract our SDs from observed rewards or transformed rewards? I've used transformed rewards here
      valuesObserved[t, rawData$deck[t]] <- utility
      
      #Update uncertainty
      if (whichLBA != 5){
        Uncs <- addUnc(bonusStyle, Uncs, test, utility, t, valuesObserved, Qs, UNCSTART[1])
      } else if (whichLBA == 5) {
        Uncs <- addUnc(1, Uncs, test, utility, t, valuesObserved, Qs, UNCSTART[1])
        tempUncs <- addUnc(3, Uncs, test, utility, t, valuesObserved, Qs, UNCSTART[1])
      }
      
    }
  }
  
  #Decision Model ----
  driftRates <- rep(0, 4)
  likelihood <- rep(0, trialNum)
  UncA <- UNCSTART
  for (t in 1:trialNum) {
    #Softmax calculation
    for (e in 1:4) {
      driftRates[e] <-  exp(QTable[t, e]/THETA) / sum(exp(QTable[t,]/THETA))
    }
    
    #Push winning Value to the front (for n1CDF)
    trialDrift <- append(driftRates, driftRates[rawData$deck[t]], after = 0)
    trialDrift <- trialDrift[-(rawData$deck[t]+1)]
    
    #Simple LBA
    if (whichLBA == 1) {
      likelihood[t] <- n1CDF(rt = rawData$rt[t], A = list(A[1], A[2], A[3], A[4]), b = B, t0 = T0, mean_v = trialDrift, sd_v = SD_V, silent = TRUE)
    } 
    #Fluctuating SD LBA
    else if (whichLBA == 2) {
      if (bonusStyle == 5) {
        SD_V[1:4] <- sqrt(STable[t, 1:4])
      } else {
        SD_V[1:4] <- UncTable[t, 1:4]/GAMMA
      }
      
      trialSD_V <- append(SD_V, SD_V[rawData$deck[t]], after = 0)
      trialSD_V <- trialSD_V[-(rawData$deck[t]+1)]
      
      likelihood[t] <- n1CDF(rt = rawData$rt[t], A = list(A[1], A[2], A[3], A[4]), b = B, t0 = T0, mean_v = trialDrift, sd_v = trialSD_V, silent = TRUE)
    }
    else if (whichLBA ==3) {
      if (bonusStyle == 5) {
        UncA[1:4] <- A + STable[t, 1:4]/GAMMA
      } else {
        # print(uncA)
        UncA[1:4] <- A + UncTable[t, 1:4]/GAMMA
      }
      for (m in 1:4) {
        if (UncA[m] > B[1]) {
          UncA[m] <- B[1]
        }
      }
      
      trialA <- append(UncA, UncA[rawData$deck[t]], after = 0)
      trialA <- trialA[-(rawData$deck[t]+1)]
      likelihood[t] <- n1CDF(rt = rawData$rt[t], A = list(trialA[1], trialA[2], trialA[3], trialA[4]), b = B, t0 = T0, mean_v = trialDrift, sd_v = SD_V, silent = TRUE)
      
    }
  }
  
  if (any(likelihood<=0)){
    holder <- data.frame(likelihood)
    holder <- filter(holder, likelihood > 0)
    for (t in 1:trialNum){

      if (likelihood[t] <= 0){

        likelihood[t] <- min(holder)
      }
    }
  }
  
  return(-sum(log(likelihood)))
}