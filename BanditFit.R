#Task which Fits the bandit task
BanditFit <- function(params, rawData, learnType){
  #Basic Details----
  trialNum <- nrow(rawData)
  S <- rep(1000, 4)
  rewardSD <- 4
  cond <- rawData$cond[1]
  
  #Assign params to values ----
  #A is the start-point for each option, let all As be different initially to account for bias
  A = rep(0, 4)
  A[1] <- params[1]
  A[2] <- params[2]
  A[3] <- params[3]
  A[4] <- params[4]
  
  #B is the bound. To keep things identifiable, make the bound set across all arms
  B <- rep(params[5], 4)
  
  #T0 is the amount of Non-decision time
  T0 <- params[6]
  
  #THETA is the temperature parameter for all models
  THETA <- params[7]
  
  #QSTART is a free parameter and helps determine whether participants will be optimistic or not at the beginning by determining starting values
  QSTART <- rep(params[8], 4)
  
  #ZETA is the prospect theory power
  ZETA <- params[9]
  
  #DELTA is the loss-aversion parameter
  DELTA <- params[10]
  
  #ALPHA_POS is the learning rate parameter for RW in the positive direction (Pedersen et al., 2017)
  ALPHA_POS <- params[11]
  #ALPHA_NEG is the learning rate parameter for RW in the negative direction (Pedersen et al., 2017)
  ALPHA_NEG <- params[12]
  
  #SD_V is the drift-rate parameter, again we'll keep this constant (unless we're testing a model where we alter it)
  SD_V <- rep(params[13], 4)
  
  #UNCSTART is where uncertainty expectations start at 
  UNCSTART <- rep(params[14], 4)
  
  #GAMMA is a general parameter that transforms uncertainty (although it will act differently in different models)
  GAMMA <- params[15]
  
  #Complete the learning model ----
  #Qs are for EVs
  Qs <- QSTART
  QTable <- matrix(ncol = 4, nrow = trialNum)
  valuesObserved <- matrix(ncol = 4, nrow = trialNum)
  
  #RW Learning ----
  if (learnType == 'RW') {
    for (t in 1:trialNum) {
      QTable[t, 1:4] <- Qs
      
      choice <- rep(0, times = length(Qs))
      choice[rawData$deck[t]] <- 1
      
      if (rawData$payoff[t] >= 0) {
        utility <- rawData$payoff[t] ^ ZETA
        Qs[1:4] <- RW(ALPHA_POS, Qs[1:4], utility, choice)
      } else {
        utility <- -DELTA * ((abs(rawData$payoff[t])) ^ ZETA)
        Qs[1:4] <- RW(ALPHA_NEG, Qs[1:4], utility, choice)
      }
      
      #Question, should we extract our SDs from observed rewards or transformed rewards? I've used transformed rewards here
      valuesObserved[t, rawData$deck[t]] <- utility
      
    }
    
    #Kalman Learning ----
  } else if (learnType == 'Kalman') {
    for (t in 1:trialNum) {
      QTable[t, 1:4] <- Qs
      
      choice <- rep(0, times = length(Qs))
      choice[rawData$deck[t]] <- 1
      
      if (rawData$payoff[t] >= 0) {
        utility <- rawData$payoff[t] ^ ZETA
      } else {
        utility <- -DELTA * ((abs(rawData$payoff[t])) ^ ZETA)
      }
      
      if ((cond == 'ntn' & t > 50 & t < 101) | 
          (cond == 'ntn' & t > 150 & t < 201) | 
          (cond == 'nts' & t > 50 & t < 101) | 
          (cond == 'nts' & t > 150 & t < 201)){
        walkSD <- 16
      } else {
        walkSD <- 4
      }
      
      outList <- KalmanFilter(Qs, S, utility, choice, walkSD, rewardSD)
      Qs <- outList$Qs
      S <- outList$S
      
      #Question, should we extract our SDs from observed rewards or transformed rewards? I've used transformed rewards here
      valuesObserved[t, rawData$deck[t]] <- utility
      
    }
  }
  
  print(QTable)
}