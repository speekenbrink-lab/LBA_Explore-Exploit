runBanditSimple = function(A, B, T0, SD_V, ALPHA, QSTART, PHI, trialNum, uncStyle) {
  #A is start-point bound
  #B is threshold
  #T0 is decision delay
  #SD_V is the standard deviation of the drift rate
  #ALPHA is the learning rate
  #PHI is the score transformation and acts like a temperature parameter
  #QSTART is the starting value of all responses
  
  #Starting variables
  SD_V <- rep(SD_V, times = 4)
  A <- rep(A, times = 4)
  S <- rep(1000, times = 4)
  
  #Qs for each action for each Cue, cols 1:4 are for response 1, cols 5:8 are for response 2
  Qs <- rep(QSTART, times = 4)
  test <- data.frame(choice = 0, rts = 0, rewards = 0, optimal = 0, tradExploit = 0)
  QTable <- matrix(ncol = 4, nrow = trialNum)
  
  #Mean rewards table on each trial
  meanRewards <- matrix(ncol = 4, nrow = trialNum + 1)
  meanRewards[1, 1:4] <- round(rnorm(4, 25, 5))
  
  #Reward walk
  walkSD <- 2
  rewardSD <- 3
  
  #Amount of eploitative decisions

  #Run Trials
  for (t in 1:trialNum){
    QTable[t, 1:4] <- Qs
    
    #Transform expected value into mean-driftrate
    driftRates <- Qs + sum(Qs) * PHI
    
    #Run LBA
    test[t, 1:2] <- LBASim(A, B, T0, driftRates, SD_V, length(Qs))
    
    if (test$choice[t] == match(max(Qs), Qs)) {
      test$tradExploit[t] <- 1
    } else {
      test$tradExploit[t] <- 0
    }
    
    #observedReward <- c(0, 0, 0, 0)
    observedReward <- rnorm(1, meanRewards[t, test$choice[t]], rewardSD) 
    if (max(meanRewards[t,]) == meanRewards[t, test$choice[t]]){
      optimal <- 1
    } else {
      optimal <-  0
    }
    
    #Walk meanRewards
    for (i in 1:4) {
      meanRewards[t+1, i] <- meanRewards[t, i] + rnorm(1, 0, walkSD)
      if (meanRewards[t+1, i] > 40) {
        meanRewards[t+1, i] <- 40
      } else if (meanRewards[t+1, i] < 10){
        meanRewards[t+1, i] <- 10
      }
    }
    
    test[t, 3] <-  observedReward
    test[t, 4] <-  optimal
    
    #Learn based on decision
    choice <- rep(0, times = length(Qs))
    choice[test$choice[t]] <- 1
    #Kalman Filter
    if (uncStyle == 5 | uncStyle == 6) {
      outList <- KalmanFilter(Qs, S, observedReward, choice, walkSD, rewardSD)
      Qs <- outList$Qs
      S <- outList$S
      #RW
    } else { 
      Qs[1:4] <- RW(ALPHA, Qs[1:4], observedReward, choice)
    }
    
  }
  
  test$choice <- as.factor(test$choice)
  
  return(test)
}