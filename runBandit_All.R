runBandit_All = function(A, B, T0, SD_V, ALPHA, QSTART, THETA, 
                         trialNum, bonusStyle, learnStyle, whichLBA,
                         A_EXPLORE, B_EXPLORE, SD_V_EXPLORE, UNCSTART,
                         OMEGA,
                         GAMMA,
                         DELTA) {
  
  #Starting variables ----
  SD_V <- rep(SD_V, times = 4)
  SD_V_EXPLORE <- rep(SD_V_EXPLORE, times = 4)
  A <- rep(A, times = 4)
  A_EXPLORE <- rep(A_EXPLORE, times = 4)
  S <- rep(1000, times = 4)
  #For SD model
  SD_Unc <- rep(0, times = 4)
  
  #Qs for each action for each Cue, cols 1:4 are for response 1, cols 5:8 are for response 2
  Qs <- rep(QSTART, times = 4)
  test <- data.frame(choice = 0, rts = 0, rewards = 0, optimal = 0, tradExploit = 0, EV_Diff = 0, Unc_Max = 0)
  QTable <- matrix(ncol = 4, nrow = trialNum)
  
  #Uncertainty Tables
  Uncs <- rep(UNCSTART, times = 4)
  tempUncs <- rep(UNCSTART, times = 4)
  UncTable <- matrix(ncol = 4, nrow = trialNum)
  
  #For SP uncertainty model
  UncA <- rep(0, times = 4)
  
  #For Two LBA Model
  exploitTable <- data.frame(choice = 0, rts = 0)
  exploreTable <- data.frame(choice = 0, rts = 0)
  
  #Mean rewards table on each trial
  meanRewards <- matrix(ncol = 4, nrow = trialNum + 1)
  meanRewards[1, 1:4] <- round(rnorm(4, 25, 5))
  
  #Reward walk
  walkSD <- 2
  rewardSD <- 3

  #Array with the values observed on each arm
  valuesObserved <- matrix(ncol = 4, nrow = trialNum)
  
  #Driftrate place holder
  driftRates <- rep(NaN, 4)
  
  
  #Run Trials----
  for (t in 1:trialNum){
    QTable[t, 1:4] <- Qs
    UncTable[t, 1:4] <- Uncs
    
    #Transform expected value into mean-driftrate
    for (e in 1:4) {
      driftRates[e] <-  exp(Qs[e]/THETA) / sum(exp(Qs/THETA))
    }
    
    #Run the different LBA models:
    #Run the Basic LBA (Option 1)
    if (whichLBA == 1) {
      
      #Run LBA
      test[t, 1:2] <- LBASim(A, B, T0, driftRates, SD_V, length(Qs))
      
      #Dual LBA
    } else if (whichLBA == 2) {
      #Transfore Uncertainty into mean-driftrate for exploration
      # UncRates <- Uncs + sum(Uncs) * DELTA
      for (e in 1:4) {
        UncRates[e] <-  exp(Uncs[e]/DELTA) / sum(exp(Uncs/DELTA))
      }
      
      #Run LBA
      exploitTable[t, 1:2] <- LBASim(A, B, T0, driftRates, SD_V, length(Qs))
      exploreTable[t, 1:2] <- LBASim(A_EXPLORE, B_EXPLORE, T0, UncRates, SD_V_EXPLORE, length(Uncs))
      
      if (exploitTable$rts[t] < exploreTable$rts[t]) {
        test[t, 1] <- exploitTable$choice[t]
        test[t, 2] <- exploitTable$rts[t]
        test$exploit[t] <- 1
        test$explore[t] <- 0
      } else {
        test[t, 1] <- exploreTable$choice[t]
        test[t, 2] <- exploreTable$rts[t]
        test$explore[t] <- 1
        test$exploit[t] <- 0
      }
      #SD Uncertainty LBA
    } else if (whichLBA == 3) {
      
      #Transfore Uncertainty into SD for driftrate
      SD_Unc[1:4] <- Uncs[1:4]/GAMMA
      
      #Run LBA
      #Number 5 is standard deviation is based on S (the variance of the mean expected value)
      if (bonusStyle == 5) {
        test[t, 1:2] <- LBASim(A, B, T0, driftRates, sqrt(S), length(Qs))
      } else {
        test[t, 1:2] <- LBASim(A, B, T0, driftRates, SD_Unc, length(Qs))
      }
      
      if (test$choice[t] == match(max(Qs), Qs)) {
        test$traditionalExploit[t] <- 1
      } else {
        test$traditionalExploit[t] <- 0
      }
      
      #SP Uncertainty LBA
    } else if (whichLBA == 4) {
      
      #Transfore Uncertainty into Start Point for drift
      #Drift rate based off Kalman Filter
      if (bonusStyle == 5) {
        UncA[1:4] <- A + S/OMEGA
        #Drift rate based off Other types of uncertainty
      } else {
        UncA[1:4] <- A + Uncs/OMEGA
      }
      for (m in 1:4) {
        if (UncA[m] > B) {
          UncA[m] <- B
        }
      }
      #Run LBA
      test[t, 1:2] <- LBASim(UncA, B, T0, driftRates, SD_V, length(Qs))
      #Combined LBA ----
    } else if (whichLBA == 5) {
      if (learnStyle == 2) {
        UncA[1:4] <- A + S/OMEGA
        #Drift rate based off Other types of uncertainty
      } else {
        #Get Uncertainty on Drift-rate from time since last picked
        UncA[1:4] <- A + Uncs/OMEGA
      }
      for (m in 1:4) {
        if (UncA[m] > B) {
          UncA[m] <- B
        }
      }
      
      SD_Unc[1:4] <- tempUncs[1:4]/GAMMA
      
      #Combine both types of simulation
      test[t, 1:2] <- LBASim(UncA, B, T0, driftRates, SD_Unc, length(Qs))
      
    }
    
    if (test$choice[t] == match(max(Qs), Qs)) {
      test$tradExploit[t] <- 1
    } else {
      test$tradExploit[t] <- 0
    }
    
    #Calculate observed reward around error and see if it was an optimal choice
    observedReward <- rnorm(1, meanRewards[t, test$choice[t]], rewardSD) 
    valuesObserved[t, test$choice] <- observedReward
    if (max(meanRewards[t,]) == meanRewards[t, test$choice[t]]){
      optimal <- 1
    } else {
      optimal <-  0
    }
    
    #Walk meanRewards
    for (i in 1:4) {
      meanRewards[t+1, i] <- meanRewards[t, i] + rnorm(1, 0, walkSD)
      if (meanRewards[t+1, i] > 40) {
        meanRewards[t+1, i] <- 40
      } else if (meanRewards[t+1, i] < 10){
        meanRewards[t+1, i] <- 10
      }
    }
    
    test[t, 3] <-  observedReward
    test[t, 4] <-  optimal
    
    #Calculate the difference between the highest option and the next highest option and store it
    test[t, 6] <- max(Qs) - max(Qs[Qs!=max(Qs)])
    if (test[t, 6] == Inf) {
      test[t, 6] <- 0
    }

    #Calculate the value of the most uncertain option (perhaps more informative?)
    test[t, 7] <- max(Uncs)
    if ((bonusStyle == 5 & whichLBA == 3) | (bonusStyle == 5 & whichLBA == 4) | bonusStyle == 5 & whichLBA == 1) {
        test[t, 7] <- max(S)
    }
    if (test[t, 7] == Inf) {
      test[t, 7] <- UNCSTART
    }

    #Update Uncertainty bonus for all those that haven't been picked. 
    if (whichLBA != 5){
      Uncs <- addUnc(bonusStyle, Uncs, test, observedReward, t, valuesObserved, Qs, UNCSTART)
    } else if (whichLBA == 5) {
      Uncs <- addUnc(1, Uncs, test, observedReward, t, valuesObserved, Qs, UNCSTART)
      tempUncs <- addUnc(3, Uncs, test, observedReward, t, valuesObserved, Qs, UNCSTART)
    }
    
    #Learn based on decision
    choice <- rep(0, times = length(Qs))
    choice[test$choice[t]] <- 1
    #Kalman Filter
    if (bonusStyle == 5 | learnStyle == 2) {
      outList <- KalmanFilter(Qs, S, observedReward, choice, walkSD, rewardSD)
      Qs <- outList$Qs
      S <- outList$S
      #RW
    } else { 
      Qs[1:4] <- RW(ALPHA, Qs[1:4], observedReward, choice)
    }
    
  }
  
  test$choice <- as.factor(test$choice)
  
  return(test)
}


 