runPart = function(A, B, T0, SD_V, ALPHA, PHI, QSTART) {
  #A is start-point bound
  #B is threshold
  #T0 is decision delay
  #SD_V is the standard deviation of the drift rate
  #ALPHA is the learning rate
  #PHI is the score transformation and acts like a temperature parameter
  #QSTART is the starting value of all responses
  
  HVR = 100 #Set value of higher rewards (options 1 and 2)
  LVR = 20 #Set value of lower rewards (options 3 and 4)
  
  #Set initial task information
  trialNum <- 448
  #Qs for each action for each Cue, cols 1:4 are for response 1, cols 5:8 are for response 2
  Qs <- rep(QSTART, times = 8)
  test <- data.frame(choice = 0, rts = 0, rewards = 0, optimal = 0)
  QTable <- matrix(ncol = 8, nrow = trialNum)
  #Trial sequence
  trialSeq <- array(c(1, 1, 2, 2, 3, 4, 3, 4), dim=c(4, 2))
  trialSeq <- matrix(rep(t(trialSeq), trialNum/4) , ncol = ncol(trialSeq), byrow = TRUE)
  #Mean values for each response given the cues
  trialVals <- rep(0, times = 2)
  
  #Run Trials
  for (t in 1:trialNum){
    #Which cues are on screen
    cuesPres <- rep(NA, times = 4)
    #Find cues in the trial
    cuesPres[trialSeq[t, 1]] <- 1
    cuesPres[trialSeq[t, 2]] <- 1
    
    QTable[t, 1:8] <- Qs
    
    #Transform presented cues for values for each choice
    trialVals[1] <- mean(cuesPres*Qs[1:4], na.rm = TRUE)
    trialVals[2] <- mean(cuesPres*Qs[5:8], na.rm = TRUE)
    
    #Transform expected value into mean-driftrate
    driftRates <- (trialVals + 0)/PHI
    
    #Run LBA
    test[t, 1:2] <- LBASim(A, B, T0, driftRates, SD_V, length(trialVals))
    
    #Get Reward dependant on choice
    #observedReward <- c(0, 0, 0, 0)
    if (test$choice[t] == 1 & 1 %in% trialSeq[t, 1:2]) {
      observedReward <- 15
      optimal <- 1
    } else if (test$choice[t] == 2 & 2 %in% trialSeq[t, 1:2]) {
      observedReward <- 15
      optimal <- 1
    } else {
      observedReward <- 10
      optimal <- 0
    }
    
    if (t > 256) {
      observedReward <- observedReward + sample(-7:7, 1)
    }
    
    test[t, 3] = observedReward
    test[t, 4] = optimal
    
    #Learn based on decision
    if (test$choice[t] == 1) {
      Qs[1:4] <- RW(ALPHA, Qs[1:4], observedReward, cuesPres)
    } else if (test$choice[t] == 2) {
      Qs[5:8] <- RW(ALPHA, Qs[5:8], observedReward, cuesPres)
    }
    
  }
  
  test$choice <- as.factor(test$choice)
  
  #Plot choices
  #ggplot(data = test, mapping = aes(x = rts, fill = choice)) +
  #geom_histogram(binwidth = .2) +
  #coord_cartesian(xlim = c(0, 10))
  return(test)
}