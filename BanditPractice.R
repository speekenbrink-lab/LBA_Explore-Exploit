#Simulate 4 armed bandit task
library('tidyverse')
setwd('C:\Users\awalk\neDrive\Documents\PhD\London\LBA practice')

#Set parameters (all in caps)
#Start-point bound
A <-5
#Threshold
B <- 10
#Decision Delay
T0 <- .5
#Standard Deviation
SD_V <- 1
#Learning Rate
ALPHA <- .3

HVR = 100 #Set value of higher rewards (options 1 and 2)
LVR = 20 #Set value of lower rewards (options 3 and 4)

#Set initial task information
trialNum <- 1000
Qs <- c(0, 0, 0 ,0)
test <- data.frame(choice = 0, rts = 0)

#Run Trials
for (t in 1:trialNum){
  #Transform expected value into mean-driftrate
  driftRates <- (Qs + 2)/10
  
  #Run LBA
  test[t, 1:2] <- LBA(A, B, T0, driftRates, SD_V, length(Qs))
  
  #Get Reward dependant on choice
  observedReward <- c(0, 0, 0, 0)
  if (test$choice[t] == 1) {
    observedReward <- c(HVR, 0, 0, 0)
  } else if (test$choice[t] == 2) {
    observedReward <- c(0, HVR, 0, 0)
  } else if (test$choice[t] == 3) {
    observedReward <- c(0, 0, LVR, 0)
  } else if (test$choice[t] == 4) {
    observedReward <- c(0, 0, 0, LVR)
  }
  
  #Learn based on decision
  Qs <- RW(ALPHA, Qs, observedReward)
}

test$choice <- as.factor(test$choice)

#Plot options
ggplot(data = test, mapping = aes(x = rts, fill = choice)) +
  geom_histogram(binwidth = .2) +
  coord_cartesian(xlim = c(0, 10))

#Learning Model
RW <- function(ALPHA, oldQs, observedValues) {
  newQs <- ALPHA * (observedValues - oldQs)
  
  return(newQs)
}
  

#RT Decision model
LBA <- function(A, B, T0, mean_v, SD_V, choiceNum) {#Simulate LBA model
  #n = number of participants
  #A = Starting point upper bound
  #b = threshold
  #t0 = start delay
  #mean_v = mean driftrate for each option
  #sd_v = mean standard deviation for each option
  
  #Simulate some trials
  
  choice <- 0
  rts <- 0
  sim <- 0
  
    trialFine <- 0
    while (!trialFine) {
      #Get Start point
      start <- runif(1, 0, A)
      drift <- 0
      time <- 0
      
      for (m in 1:choiceNum) {
      #Get Drift Rates
        drift[m] <- rnorm(1, mean_v[m], SD_V)
        
        #Get time to threshold
        time[m] <- (B - start) / drift[m]
        
        #Add non-decision time
        time[m] = time[m] + T0
      }
      
      #Check we don't have negative times (Make sure to change this to correspond to the number of choices available)
      if (sum(time > 0) == length(time)) {
        trialFine = 1
      }
    }
    
    #Make choice based on values
    choice <- which.min(time)
    rts <- min(time)
  
  sim <- data.frame(choice, rts)
  
  return(sim)
}

