#Different bonus styles:
#1 is simply how long since the option was last picked
runBandit_TwoLBA = function(A, B, T0, SD_V, ALPHA, PHI, QSTART, UNCSTART, GAMMA, trialNum, bonusStyle) {
  #A is start-point bound
  #B is threshold
  #T0 is decision delay
  #SD_V is the standard deviation of the drift rate
  #ALPHA is the learning rate
  #PHI is the score transformation and acts like a temperature parameter
  #QSTART is the starting value of all responses
  #UNCSTART is the starting uncertainty bias
  #GAMMA is the weighting parameter for uncertainty
  
  #Qs for each action for each Cue, cols 1:4 are for response 1, cols 5:8 are for response 2
  Qs <- rep(QSTART, times = 4)
  exploitTable <- data.frame(choice = 0, rts = 0)
  QTable <- matrix(ncol = 4, nrow = trialNum)
  #Measure of uncertainty, can try both time since last picked, and also standard deviation 
  Uncs <- rep(UNCSTART, times = 4)
  UncTable <- matrix(ncol = 4, nrow = trialNum)
  exploreTable <- data.frame(choice = 0, rts = 0)
  
  #Final table
  test <- data.frame(choice = 0, rts = 0, rewards = 0, optimal = 0, traditionalExploit = 0, exploit = 0, explore = 0)
  
  #Mean rewards table on each trial
  meanRewards <- matrix(ncol = 4, nrow = trialNum + 1)
  meanRewards[1, 1:4] <- round(rnorm(4, 25, 5))
  
  #Reward walk
  rewardSD <- 2
  
  #Amount of eploitative decisions
  
  #Run Trials
  for (t in 1:trialNum){
    QTable[t, 1:4] <- Qs
    UncTable[t, 1:4] <- Uncs
    
    #Transform expected value into mean-driftrate or exploitation
    driftRates <- Qs/PHI
    
    #Transfore Uncertainty into mean-driftrate for exploration
    UncRates <- Uncs/GAMMA
    
    #Run LBA
    exploitTable[t, 1:2] <- LBASim(A, B, T0, driftRates, SD_V, length(Qs))
    exploreTable[t, 1:2] <- LBASim(A, B, T0, UncRates, SD_V, length(Uncs))
    
    if (exploitTable$rts[t] < exploreTable$rts[t]) {
      test[t, 1] <- exploitTable$choice[t]
      test[t, 2] <- exploitTable$rts[t]
      test$exploit[t] <- 1
      test$explore[t] <- 0
    } else {
      test[t, 1] <- exploreTable$choice[t]
      test[t, 2] <- exploreTable$rts[t]
      test$explore[t] <- 1
      test$exploit[t] <- 0
    }
    
    if (test$choice[t] == match(max(Qs), Qs)) {
      test$traditionalExploit[t] <- 1
    } else {
      test$traditionalExploit[t] <- 0
    }
    
    #observedReward <- c(0, 0, 0, 0)
    observedReward <- meanRewards[t, test$choice[t]]
    if (max(meanRewards[t,]) == observedReward){
      optimal <- 1
    } else {
      optimal <-  0
    }
    
    #Walk meanRewards
    for (i in 1:4) {
      meanRewards[t+1, i] <- meanRewards[t, i] + rnorm(1, 0, rewardSD)
      if (meanRewards[t+1, i] > 40) {
        meanRewards[t+1, i] <- 40
      } else if (meanRewards[t+1, i] < 10){
        meanRewards[t+1, i] <- 10
      }
    }
    
    test[t, 3] <-  observedReward
    test[t, 4] <-  optimal
    
    #Learn based on decision
    choice <- rep(0, times = length(Qs))
    choice[test$choice[t]] <- 1
    Qs[1:4] <- RW(ALPHA, Qs[1:4], observedReward, choice)
    
    #Update Uncertainty bonus for all those that haven't been picked. 
    if (bonusStyle == 1){
      Uncs <- Uncs + 1
      Uncs[test$choice[t]] <- 1
    }
    
  }
  
  test$choice <- as.factor(test$choice)
  
  return(test)
}