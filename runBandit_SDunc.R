#Different bonus styles:
#1 is simply how long since the option was last picked
#2 is the observed standard deviation on each option across all scores
#3 is the observed standard deviation across all rewards
#4 is the prediction error from the last trial. 
runBandit_SDunc = function(A, B, T0, ALPHA, PHI, QSTART, UNCSTART, GAMMA, trialNum, bonusStyle) {
  #A is start-point bound
  #B is threshold
  #T0 is decision delay
  #SD_V is the standard deviation of the drift rate
  #ALPHA is the learning rate
  #PHI is the score transformation and acts like a temperature parameter
  #QSTART is the starting value of all responses
  #UNCSTART is the starting uncertainty bias
  #GAMMA is the weighting parameter for uncertainty
  
  #Qs for each action for each Cue, cols 1:4 are for response 1, cols 5:8 are for response 2
  Qs <- rep(QSTART, times = 4)
  QTable <- matrix(ncol = 4, nrow = trialNum)
  #Measure of uncertainty, can try both time since last picked, and also standard deviation 
  Uncs <- rep(UNCSTART, times = 4)
  UncTable <- matrix(ncol = 4, nrow = trialNum)
  SD_Unc <- rep(0, times = 4)
  
  #Array with the values observed on each arm
  valuesObserved <- matrix(ncol = 4, nrow = trialNum)
  
  #Final table
  test <- data.frame(choice = 0, rts = 0, rewards = 0, optimal = 0, traditionalExploit = 0, exploit = 0, explore = 0)
  
  #Mean rewards table on each trial
  meanRewards <- matrix(ncol = 4, nrow = trialNum + 1)
  meanRewards[1, 1:4] <- round(rnorm(4, 25, 5))
  
  #Reward walk
  rewardSD <- 2
  
  #Amount of eploitative decisions
  
  #Run Trials
  for (t in 1:trialNum){
    QTable[t, 1:4] <- Qs
    UncTable[t, 1:4] <- Uncs
    
    #Transform expected value into mean-driftrate
    driftRates <- Qs/PHI
    
    #Transfore Uncertainty into SD for driftrate
    SD_Unc[1:4] <- Uncs[1:4]/GAMMA
    
    
    
    #Run LBA
    test[t, 1:2] <- LBASim(A, B, T0, driftRates, SD_Unc, length(Qs))
    
    if (test$choice[t] == match(max(Qs), Qs)) {
      test$traditionalExploit[t] <- 1
    } else {
      test$traditionalExploit[t] <- 0
    }
    
    #observedReward <- c(0, 0, 0, 0)
    observedReward <- meanRewards[t, test$choice[t]]
    valuesObserved[t, test$choice] <- observedReward
    
    if (max(meanRewards[t,]) == observedReward){
      optimal <- 1
    } else {
      optimal <-  0
    }
    
    #Walk meanRewards
    for (i in 1:4) {
      meanRewards[t+1, i] <- meanRewards[t, i] + rnorm(1, 0, rewardSD)
      if (meanRewards[t+1, i] > 40) {
        meanRewards[t+1, i] <- 40
      } else if (meanRewards[t+1, i] < 10){
        meanRewards[t+1, i] <- 10
      }
    }
    
    test[t, 3] <-  observedReward
    test[t, 4] <-  optimal
    
    #Update Uncertainty bonus for all those that haven't been picked. 
    if (bonusStyle == 1){
      Uncs <- Uncs + 1
      Uncs[test$choice[t]] <- 1
    } else if (bonusStyle == 2) {
      Uncs[test$choice[t]] <- sd(valuesObserved[,test$choice[t]], na.rm = TRUE)  
      if (is.na(Uncs[test$choice[t]])) {
        Uncs[test$choice[t]] <- UNCSTART
      }
    } else if (bonusStyle == 3) {
      Uncs[1:4] <- sd(test$rewards, na.rm = TRUE)
      if (is.na(Uncs[1])) {
        Uncs[1:4] <- UNCSTART
      }
    } else if (bonusStyle == 4) {
      Uncs[1:4] <- abs(observedReward - Qs[test$choice[t]])
    }
    
    #Learn based on decision
    choice <- rep(0, times = length(Qs))
    choice[test$choice[t]] <- 1
    Qs[1:4] <- RW(ALPHA, Qs[1:4], observedReward, choice)
    
  }
  
  test$choice <- as.factor(test$choice)
  
  return(test)
}