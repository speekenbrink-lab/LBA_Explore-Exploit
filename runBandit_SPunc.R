runBandit_SPunc = function(A, B, T0, SD_V, ALPHA, QSTART, PHI, 
                           UNCSTART, OMEGA, trialNum, bonusStyle, learnStyle) {
  #A is start-point bound
  #B is threshold
  #T0 is decision delay
  #SD_V is the standard deviation of the drift rate
  #ALPHA is the learning rate
  #PHI is the score transformation and acts like a temperature parameter
  #QSTART is the starting value of all responses
  #UNCSTART is the starting uncertainty bias
  #OMEGA is the scaling parameter for uncertainty
  
  #Starting Variables
  SD_V <- rep(SD_V, times = 4)
  A <- rep(A, times = 4)
  S <- rep(1000, times = 4)
  
  #Qs for each action for each Cue, cols 1:4 are for response 1, cols 5:8 are for response 2
  Qs <- rep(QSTART, times = 4)
  QTable <- matrix(ncol = 4, nrow = trialNum)
  #Measure of uncertainty, can try both time since last picked, and also standard deviation 
  Uncs <- rep(UNCSTART, times = 4)
  UncTable <- matrix(ncol = 4, nrow = trialNum)
  UncA <- rep(0, times = 4)
  
  #Array with the values observed on each arm
  valuesObserved <- matrix(ncol = 4, nrow = trialNum)
  
  #Final table
  test <- data.frame(choice = 0, rts = 0, rewards = 0, optimal = 0, traditionalExploit = 0, exploit = 0, explore = 0)
  
  #Mean rewards table on each trial
  meanRewards <- matrix(ncol = 4, nrow = trialNum + 1)
  meanRewards[1, 1:4] <- round(rnorm(4, 25, 5))
  
  #Reward walk
  walkSD <- 2
  rewardSD <- 3
  
  #Amount of eploitative decisions
  
  #Run Trials
  for (t in 1:trialNum){
    QTable[t, 1:4] <- Qs
    UncTable[t, 1:4] <- Uncs
    
    #Transform expected value into mean-driftrate
    driftRates <- Qs + sum(Qs) * PHI
    
    #Transfore Uncertainty into Start Point for drift
    #Drift rate based off Kalman Filter
    if (bonusStyle == 5) {
      UncA[1:4] <- A + S/OMEGA
    #Drift rate based off Other types of uncertainty
    } else {
      UncA[1:4] <- A + Uncs/OMEGA
    }
    for (m in 1:4) {
      if (UncA[m] > B) {
        UncA[m] <- B
      }
    }
    
    #Run LBA
    test[t, 1:2] <- LBASim(UncA, B, T0, driftRates, SD_V, length(Qs))
    
    if (test$choice[t] == match(max(Qs), Qs)) {
      test$traditionalExploit[t] <- 1
    } else {
      test$traditionalExploit[t] <- 0
    }
    
    #observedReward <- c(0, 0, 0, 0)
    #Return a reward from a distribution
    observedReward <- rnorm(1, meanRewards[t, test$choice[t]], rewardSD)
    valuesObserved[t, test$choice] <- observedReward
    if (max(meanRewards[t,]) == meanRewards[t, test$choice[t]]){
      optimal <- 1
    } else {
      optimal <-  0
    }
    
    #Walk meanRewards
    for (i in 1:4) {
      meanRewards[t+1, i] <- meanRewards[t, i] + rnorm(1, 0, walkSD)
      if (meanRewards[t+1, i] > 40) {
        meanRewards[t+1, i] <- 40
      } else if (meanRewards[t+1, i] < 10){
        meanRewards[t+1, i] <- 10
      }
    }
    
    test[t, 3] <-  observedReward
    test[t, 4] <-  optimal
    
    #Update Uncertainty bonus for all those that haven't been picked. 
    Uncs <- addUnc(bonusStyle, Uncs, test, observedReward, t)
    
    #Learn based on decision
    choice <- rep(0, times = length(Qs))
    choice[test$choice[t]] <- 1
    #Kalman Filter
    if (bonusStyle == 5 | learnStyle == 2) {
      outList <- KalmanFilter(Qs, S, observedReward, choice, walkSD, rewardSD)
      Qs <- outList$Qs
      S <- outList$S
      #RW
    } else { 
      Qs[1:4] <- RW(ALPHA, Qs[1:4], observedReward, choice)
    }
  }
  
  test$choice <- as.factor(test$choice)
  
  return(test)
}